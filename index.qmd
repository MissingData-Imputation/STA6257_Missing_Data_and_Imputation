---
title: "Missing Data and Imputation"
author:
  - Javier Estrada
  - Michael Underwood
  - Elizabeth Subject-Scott
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
    css: styles.css
    theme: morph
    toc: true
    toc-location: left
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
editor: 
  markdown: 
    wrap: 72
---

[Website](https://missingdata-imputation.github.io/STA6257_Missing_Data_and_Imputation/)

[Slides](https://rstudio.hmcse.uwf.edu/s/4c3307ed3545b19414f79/files/STA6257_Missing_Data_and_Imputation/Slides.html#/title-slide)

## Introduction

#### What is missing data?

Missing data occurs when there are missing values in a dataset. There
are many reasons why this occurs. It can be intentional or unintentional
and can be classified into the following three categories, otherwise
known as missingness mechanisms [@mai2023]:

-   Missing completely at random (MCAR) is the probability of missing
    data being completely independent of any other variables.

-   Missing at random (MAR) is the probability of missing data being
    related to the observed values.

-   Missing not at random (MNAR) is the probability of missing data
    being dependent on the missing and observed values.

The problem with ignoring missing data is that it does not give a true
representation of the dataset and can lead to bias when analyzing. This
reduces the statistical power of the analysis [@van2020]. To enhance the
quality of the research, the following should be followed: explicitly
acknowledge missing data problems and the conditions under which they
occur and employ principled methods to handle missing data [@don2013].
There are two ways to handle missing values: delete or impute.

#### Deleting missing data

Listwise deletion is when the entire observation is removed from the
dataset. Deleting missing data can lead to the loss of important
information regarding your dataset and is therefore not recommended. In
certain cases, when the amount of missing data is small and the type is
MCAR, listwise deletion can be used. There usually won't be bias but you
are losing some information which may be important.

T-tests and chi-square tests can be used to assess pairs of predictor
variables to determine whether the groups' means differ significantly.
If significant, the null hypothesis is rejected; therefore, missing data
is not randomly scattered throughout the data [@van2020]. This implies
that the missing data is MAR or MNAR. If nonsignificant, this implies
that the data cannot be MAR (cannot eliminate possibility that it is not
MNAR--other information about the population is needed). Listwise
deletion would be wasteful and analysis biased if data is MAR or MNAR.
Alternate methods of dealing with the missing data is recommended:
either pairwise deletion or imputation.

Pairwise deletion is when only the missing variable of an observation is
removed. It allows more data to be analyzed than listwise deletion but
limits the ability to make inferences of the total sample. For this
reason, it is recommended to use imputation to properly deal with
missing data.

#### What is imputation?

Imputation is the method of replacing missing data with an estimate
obtained from the original available data so there is a full data set to
analyze. To improve statistical power, the number of imputations created
should be at least equal to the percent of missing data (5% equals 5
imputations, 10% equals 10 imputations, 20% equals 20 imputations, etc.)
[@ped2017]. There are two types of imputation: single and multiple.

## Imputation Methods

#### Single Imputation

Single or univariate imputation is when only one estimate is used to
replace the missing data. Methods of single imputation include using the
mean, the last observation carried forward, and random imputation. The
following is a brief explanation of each:

-   Using the mean to replace a missing value is a straight-forward
    process. The mean of the dataset is calculated, including the
    missing value. The mean is then multiplied by the number of
    observations in the study. Next, the known values are subtracted
    from the product, and this gives you the missing value.

-   Last Observation Carried Forward (LOCF) is a tehcnique of replacing
    a missing value in longitudinal studies with a previously observed
    value (the most recent value is carried forward) [@str2008]. The
    problem with this is that it assumes that the previous observed
    value is perpetual when in reality there is a difference. This is a
    flawed method and causes bias.

-   Random imputation is a method of randomly drawing an observation and
    using that observation for any missing values. The problem with this
    is that it introduces additional variability.

These single imputation methods often result in underestimation of
standard errors or too small p-values [@don2013]; therefore, multiple
imputation is the better method because it handles missing data better
and provides less biased results.

#### Multiple Imputation

Multiple imputation is an imputation method for missing data which imputes the missing values with random values
to generate m-amount of datasets with complete cases.

For the purpose of this project, we uitilized Multiple Imputation (MI)
for the credit dataset (dataset consists of 14 variables: status, home, time, age, marital, records, job, expenses,
income, assets, debt, amount, price). 

In R which is a statistical programming software, we created 5 datasets with imputed values using the MICE package 
(Multivariate Imputation by Chained Equations) which can seamlessly impute missing values in a dataset
by looking at the data from other columns and estimate the best prediction for each missing value.

```{r, warning=FALSE, echo=T, message=FALSE}
#loading packages
library(mice)
```



Multiple or multivariate imputation is when various estimates are used
to replace the missing data by creating multiple datasets from versions
of the original dataset. It can be done by using a regression model, or
a sequence of regression models, such as linear, logistic and Poison. A
set of *m* plausible values are generated for each unobserved data
point, resulting in *m* complete data sets [@don2013]. The new values
are randomly drawn from predictive distributions either through joint
modeling (JM, which is not used much anymore) or fully conditional
specification (FCS) [@won2023]. It is then analyzed and the results are
combined to obtain a single value for the missing data.

The purpose of multiple imputation is to create a pool of imputed data
for analysis, but if the pooled results are lacking, then multiple
imputation should not be done [@mai2023]. Another reason not to use
multiple imputation is if there are very few missing values; there may
be no benefit in using it. Also worth noting is some statistical
analyses software already have built-in features to deal with missing
data.

Figure 1: Flowchart of the MICE-process based on procedures proposed by
Rubin [@wul2017]:

```{r, warning=FALSE, echo=T, message=FALSE}
#loading packages
library(DiagrammeR)
```

```{r}
DiagrammeR::grViz("digraph {

# initiate graph
graph [layout = dot, rankdir = LR, label = 'The MICE-Process\n\n',labelloc = t, fontcolor = DarkSlateBlue]

# global node settings
node [shape = rectangle, style = filled, fillcolor = AliceBlue, fontcolor = DarkSlateBlue]
bgcolor = none

# label nodes
incomplete [label =  'Incomplete data set']
imputed1 [label = 'Imputed \n data set 1']
estimates1 [label = 'Estimates from \n analysis 1']
rubin [label = 'Rubin rules', shape = diamond]
combined [label = 'Combined results']
imputed2 [label = 'Imputed \n data set 2']
estimates2 [label = 'Estimates from \n analysis 2']
imputedm [label = 'Imputed \n data set m']
estimatesm [label = 'Estimates from \n anaalysis m']


# edge definitions with the node IDs
incomplete -> imputed1 [arrowhead = vee, color = DarkSlateBlue]
imputed1 -> estimates1 [arrowhead = vee, color = DarkSlateBlue]
estimates1 -> rubin [arrowhead = vee, color = DarkSlateBlue]
incomplete -> imputed2 [arrowhead = vee, color = DarkSlateBlue]
imputed2 -> estimates2 [arrowhead = vee, color = DarkSlateBlue]
estimates2-> rubin [arrowhead = vee, color = DarkSlateBlue]
incomplete -> imputedm [arrowhead = vee, color = DarkSlateBlue]
imputedm -> estimatesm [arrowhead = vee, color = DarkSlateBlue]
estimatesm -> rubin [arrowhead = vee, color = DarkSlateBlue]
rubin -> combined [arrowhead = vee, color = DarkSlateBlue]
}")
```

Before imputing the missing data, it is important to check whether the
data has a linear relationship. Nonlinear data should be accounted for
in the analysis as well as the imputation process. Otherwise, an
incorrect statistical model will result. It is also important to check
for anomalies that may occur during imputation. Imputation models may
need to be adjusted if the imputed values fall outside the minimum and
maximum range of the observed values. More or fewer variables in the
imputation model may be needed for these types of anomalies.

##### Step 1

Evaluate data and select independent variables

##### Step 2

Create m sets of data from original dataset with missing values

##### Step 3

Perform analysis on each of the m sets

##### Step 4

Average the estimates across m estimates. Calculate the standard errors
and variance of m estimates. Combine using an adjustment term (1+1/m).

#### Other Methods of Imputation

Regression Imputation [@van2020]

Predictive Mean Matching [@van2020]

Hot Deck (HD) [@tho2022]

Stochastic Regression (SR) [@tho2022]

Random Forest (RF) [@tho2022]

#### Alternative Methods to Impuation

Likelihood and Bayesian Methods [@cao2021]

Weighted Methods [@cao2021]


## Analysis and Results

## Data and Visualization

```{r}
# Load data
credit = read.csv("credit_data.csv")
```

##### Description

Credit score data

##### Details

The credit_data.csv file contains data of 4454 subjects and stores a
combination of continuous, categorical and count values for 15
variables. Of the 15 variables, the "Status" variable contains binomial
categorical values of "good" and "bad" to describe the kind of credit
score each subject has.

##### Structure:

```{r}
str(credit)
```

##### Statistical summary:

```{r}
summary(credit)
```

##### Diagnostics

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(ggfortify)
```

```{r, warning=FALSE, echo=TRUE}
model1=lm(Amount~Status+Seniority+Home+Time+Age+Marital+Records+Job+Expenses+Income+Assets+Debt+Price, data=credit)
autoplot(model1)
# ggplot1 = credit %>% ggplot(mapping = aes(x=population/10^6, y=total)) 
# 
#   ggplot1 + geom_point(aes(col=region), size = 4) +
#   geom_text_repel(aes(label=abb)) +
#   scale_x_log10() +
#   scale_y_log10() +
#   geom_smooth(formula = "y~x", method=lm,se = F)+
#   xlab("Populations in millions (log10 scale)") + 
#   ylab("Total number of murders (log10 scale)") +
#   ggtitle("US Gun Murders in 2010") +
#   scale_color_discrete(name = "Region")+
#       theme_wsj()
```

## Statistical Modeling

## Conclusion

In conclusion, missing data can occur in research for a variety of
reasons. It is never a good idea to ignore it. Doing this will lead to
biased estimates of parameters, loss of information, decreased
statistical power, and weak reliability of findings [@don2013]. The best
course of action is to impute the missing data by using multiple
imputation. When missing data is discovered, it is important to first
identify it and look for missing data patterns. Next, define the
variables in the dataset that are related to the missing values that
will be used for imputation. Create the necessary number of complete
data sets. Run the models and combine them using the imputed values, and
finally, analyze the complete dataset. Performing these steps will
minimize the adverse effects caused by missing data on the anaylsis
[@pam2016].
